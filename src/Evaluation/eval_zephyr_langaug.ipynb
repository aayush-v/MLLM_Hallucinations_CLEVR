{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3cef6dbc-a2b4-41b7-9d8a-387af92de868",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83290ff5-d72d-45c6-9f8d-1e117e63b167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeEvalsToFile(EVAL_PATH, RESULTS_PATH):\n",
    "    files = glob.glob(os.path.join(EVAL_PATH, '*.json'), recursive=True)\n",
    "    compiled_results = {}\n",
    "    \n",
    "    for path in files:\n",
    "        operation = path.split('_')[-1][:-5]\n",
    "        results_file = open(os.path.join(RESULTS_PATH, f\"{path.split('/')[-1]}\"), 'w')\n",
    "        results = {}\n",
    "        check_file = os.path.getsize(path)\n",
    "\n",
    "        if check_file == 0:\n",
    "            continue\n",
    "\n",
    "        with open(path, 'r') as file:\n",
    "            evals = json.load(file)\n",
    "            total_correct = 0\n",
    "            total_incorrect = 0\n",
    "            unknown = 0\n",
    "\n",
    "\n",
    "            yes_idx = []\n",
    "            no_idx = []\n",
    "            \n",
    "            for idx, eval in enumerate(evals):\n",
    "                ground_truth = eval['ground_truth']\n",
    "                if ground_truth == 'yes':\n",
    "                    yes_idx.append(idx)\n",
    "                else:\n",
    "                    no_idx.append(idx)\n",
    "                    \n",
    "            seed_value = 42\n",
    "            np.random.seed(seed_value)\n",
    "            \n",
    "            # Determine the size of the smaller array\n",
    "            min_size = min(len(yes_idx), len(no_idx))\n",
    "            \n",
    "            # Select equal number of elements from both arrays\n",
    "            selected_elements_array1 = np.random.choice(yes_idx, min_size, replace=False)\n",
    "            selected_elements_array2 = np.random.choice(no_idx, min_size, replace=False)\n",
    "            \n",
    "            for idx, eval in enumerate(evals):\n",
    "\n",
    "                if idx not in selected_elements_array1 and idx not in selected_elements_array2:\n",
    "                    continue\n",
    "                    \n",
    "                ground_truth = eval['ground_truth']\n",
    "                answer_type = 'yes-no'\n",
    "                consistent = eval['consistent'].split('consistent:')[-1]\n",
    "                is_correct = \"unknown\"\n",
    "\n",
    "                if 'yes' in consistent.lower() or 'true' in consistent.lower():\n",
    "                    is_correct = \"correct\"\n",
    "                    total_correct += 1\n",
    "                elif 'no' in consistent.lower() or 'false' in consistent.lower():\n",
    "                    is_correct = \"incorrect\"\n",
    "                    total_incorrect += 1\n",
    "                else:\n",
    "                    unknown += 1\n",
    "\n",
    "                key = f\"{operation}_{ground_truth}_{is_correct}\"\n",
    "\n",
    "                results[key] = results.get(key, 0) + 1\n",
    "                compiled_results[key] = compiled_results.get(key, 0) + 1\n",
    "\n",
    "\n",
    "            results['prediction'] = {}\n",
    "            results['prediction']['correct'] = total_correct\n",
    "            results['prediction']['incorrect'] = total_incorrect\n",
    "            results['prediction']['unknown'] = unknown\n",
    "\n",
    "            # sorted_items = sorted(results.items(), key=lambda x: x[0])\n",
    "            # sorted_dict = dict(sorted_items)\n",
    "\n",
    "            if total_correct + total_incorrect > 0:\n",
    "                results[\"accuracy\"] = total_correct/(total_correct+total_incorrect)\n",
    "\n",
    "            json.dump(results, results_file)\n",
    "            results_file.close()\n",
    "\n",
    "\n",
    "        compiled_results_file = open(os.path.join(RESULTS_PATH, \"new_compiled_results.json\"), 'w')\n",
    "          \n",
    "        json.dump(compiled_results, compiled_results_file)\n",
    "        compiled_results_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "90190ce7-dfa3-4d5e-86bc-06d394cc12e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showEvalsPlotOver_( mllm, comparison = 'num'):\n",
    "    lang_augmentations = ['orig', 'and', 'or', 'not', 'complex']\n",
    "    results = [[] for _ in range(5)]\n",
    "\n",
    "    if comparison == 'num':\n",
    "        iterate = range(3,11)\n",
    "    elif comparison == 'type':\n",
    "        iterate = ['inter', 'intra']\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    for i in iterate:\n",
    "        for idx, aug in enumerate(lang_augmentations):\n",
    "            with open(f\"{RESULTS_PATH}/val_{comparison}_{i}_{aug}.json\", 'r') as file:\n",
    "                data = json.load(file)    \n",
    "                results[idx].append(data)\n",
    "    \n",
    "    x = iterate\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    for idx, aug in enumerate(lang_augmentations):\n",
    "        plt.plot(x, [res['accuracy'] for res in results[idx]], marker='o', label=f'accuracy_{aug}')\n",
    "    \n",
    "    plt.xlabel('Object')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title(f'Accuracy Variation over Objects - Language Augmentation ({mllm})')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(f'{RESULTS_PATH}/plot_{comparison}.pdf')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc92118-7450-4eaa-a5ff-92f4033f8996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------PLOTS FOR LLaVA-------\n",
      "-------PLOTS FOR LLaMA-------\n",
      "-------PLOTS FOR InstructBLIP-------\n"
     ]
    }
   ],
   "source": [
    "MLLM_LIST = ['LLaVA', 'LLaMA', 'InstructBLIP']\n",
    "\n",
    "for mllm in MLLM_LIST:\n",
    "    EVAL_PATH = f'/scratch/averma90/MLLM_Hallucinations_CLEVR/outputs/language_augmentation/Zephyr_Results/{mllm}'  \n",
    "    RESULTS_PATH = f'/scratch/averma90/MLLM_Hallucinations_CLEVR/outputs/language_augmentation/Zephyr_Analysis/{mllm}'\n",
    "    \n",
    "    writeEvalsToFile(EVAL_PATH, RESULTS_PATH)\n",
    "    print(f'-------PLOTS FOR {mllm}-------')\n",
    "    # showEvalsPlotOver_(mllm,'num')\n",
    "    # showEvalsPlotOver_(mllm, 'type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88df9f3-1a32-4fa1-a8ab-402c3fb20faa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
